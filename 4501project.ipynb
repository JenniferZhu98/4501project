{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bd2e04",
   "metadata": {},
   "source": [
    "# IEOR4501 Final Project\n",
    "#### Zhengyi Zhu (zz2875), Binghao Guo (bg2781), Mike Cellini (mjc2328)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113ace4",
   "metadata": {},
   "source": [
    "## Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbed6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "import bs4\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import sqlite3\n",
    "import sqlalchemy as db\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fbafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_DATA_DIR = \"taxi_data\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"weather\"\n",
    "\n",
    "# random seed for reproducibility\n",
    "random.seed(1)\n",
    "\n",
    "# coordinate reference system\n",
    "CRS = 4326\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3d1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c055ee",
   "metadata": {},
   "source": [
    "# Part I: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a30dcc2",
   "metadata": {},
   "source": [
    "## Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8653fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get taxi zones data as a zip file\n",
    "def get_taxi_zones_zip():\n",
    "    TAXI_ZONE_URL = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\"\n",
    "    try: # first check whether the file already exists\n",
    "        with open(\"taxi_zones.zip\", \"r\") as f:\n",
    "            pass\n",
    "    except: # if the file doesn't exist, then download and save the zip file\n",
    "        response = requests.get(TAXI_ZONE_URL)\n",
    "        with open(\"taxi_zones.zip\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20d28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download taxizones.zip from url using requests, and manually unzip the files\n",
    "get_taxi_zones_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f1a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load taxi_zones shapefile using geopandas\n",
    "def load_taxi_zones(shapefile):\n",
    "    taxi_zones_shapefile = gpd.read_file(shapefile)\n",
    "    return taxi_zones_shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6142227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and save taxi_zones shapefile in \"loaded_taxi_zones\"\n",
    "loaded_taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accf0df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((1033269.244 172126.008, 103343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((1026308.770 256767.698, 1026495.593 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((992073.467 203714.076, 992068.667 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.092146</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>POLYGON ((935843.310 144283.336, 936046.565 14...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
       "0         1    0.116357    0.000782           Newark Airport           1   \n",
       "1         2    0.433470    0.004866              Jamaica Bay           2   \n",
       "2         3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
       "3         4    0.043567    0.000112            Alphabet City           4   \n",
       "4         5    0.092146    0.000498            Arden Heights           5   \n",
       "\n",
       "         borough                                           geometry  \n",
       "0            EWR  POLYGON ((933100.918 192536.086, 933091.011 19...  \n",
       "1         Queens  MULTIPOLYGON (((1033269.244 172126.008, 103343...  \n",
       "2          Bronx  POLYGON ((1026308.770 256767.698, 1026495.593 ...  \n",
       "3      Manhattan  POLYGON ((992073.467 203714.076, 992068.667 20...  \n",
       "4  Staten Island  POLYGON ((935843.310 144283.336, 936046.565 14...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check loaded_taxi_zones\n",
    "print(loaded_taxi_zones.shape)\n",
    "loaded_taxi_zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1d70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coords_for_taxi_zones(loaded_taxi_zones):\n",
    "    geo_col = loaded_taxi_zones[\"geometry\"].to_crs(4326)\n",
    "    loaded_taxi_zones[\"latitude\"] = geo_col.centroid.y\n",
    "    loaded_taxi_zones[\"longitude\"] = geo_col.centroid.x\n",
    "    return loaded_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4590d325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3944803749.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  loaded_taxi_zones[\"latitude\"] = geo_col.centroid.y\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3944803749.py:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  loaded_taxi_zones[\"longitude\"] = geo_col.centroid.x\n"
     ]
    }
   ],
   "source": [
    "# add latitude and longitude columns to loaded_taxi_zones, then show the top lines\n",
    "loaded_taxi_zones = add_coords_for_taxi_zones(loaded_taxi_zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86fdf40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select relevant columns, drop duplicate columns\n",
    "loaded_taxi_zones = loaded_taxi_zones[[\"LocationID\", \"latitude\", \"longitude\"]]\n",
    "loaded_taxi_zones.drop_duplicates(subset = [\"LocationID\"], inplace = True)\n",
    "loaded_taxi_zones.sort_values(by = [\"LocationID\"], inplace = True)\n",
    "loaded_taxi_zones.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b357c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>40.691831</td>\n",
       "      <td>-74.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>40.616745</td>\n",
       "      <td>-73.831299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>40.864474</td>\n",
       "      <td>-73.847422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.723752</td>\n",
       "      <td>-73.976968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40.552659</td>\n",
       "      <td>-74.188484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID   latitude  longitude\n",
       "0           1  40.691831 -74.174000\n",
       "1           2  40.616745 -73.831299\n",
       "2           3  40.864474 -73.847422\n",
       "3           4  40.723752 -73.976968\n",
       "4           5  40.552659 -74.188484"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cleaned loaded_taxi_zones\n",
    "print(loaded_taxi_zones.shape)\n",
    "loaded_taxi_zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34b5214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute latitude & longtitude from zone id\n",
    "def lookup_coords_for_taxi_zone_id(zone_loc_id):\n",
    "    rows = loaded_taxi_zones[loaded_taxi_zones[\"LocationID\"] == zone_loc_id]\n",
    "    lon = rows[\"longitude\"].mean()\n",
    "    lat = rows[\"latitude\"].mean()\n",
    "    return (lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30521776",
   "metadata": {},
   "source": [
    "## Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c9689f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance with start and end location coords\n",
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    d2 = (from_coord[0] - to_coord[0])**2 + (from_coord[1] - to_coord[1])**2\n",
    "    d = math.sqrt(d2)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316f551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append \"distance\" column to dataframe\n",
    "def add_distance_column(dataframe):\n",
    "    dt_sub = dataframe[[\"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\"]]\n",
    "    d = dt_sub.apply(lambda x: calculate_distance_with_coords(tuple([x[\"start_lat\"], x[\"start_lon\"]]), \\\n",
    "                                                              tuple([x[\"end_lat\"], x[\"end_lon\"]])), \\\n",
    "                     axis=1)\n",
    "    dataframe[\"distance\"] = d\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a868f14",
   "metadata": {},
   "source": [
    "## Auxiliary functions to help clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712fd2c",
   "metadata": {},
   "source": [
    "A __\"valid\" month__ should be within __2009-01__ to __2015-06__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acf562e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to check whether the given year and month is within range\n",
    "def isValidYM(yr, m): \n",
    "    valid = False\n",
    "    if (yr>=2009) & (yr<=2014):\n",
    "        if (m>=1) & (m<=12):\n",
    "            valid = True\n",
    "    if (yr==2015):\n",
    "        if (m>=1) & (m<=6):\n",
    "            valid = True\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4c07b",
   "metadata": {},
   "source": [
    "A __\"valid\" trip__ should satisfy the following requirements:\n",
    "1. start_location and end_location are __different__. (Otherwise, the trip distance is 0. )\n",
    "2. start_location and end_location are __within the range of New York__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0307fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to check whether the given trip is valid with start and end location coords\n",
    "def isValidTrip_coords(from_coord, to_coord):\n",
    "    valid = False\n",
    "    \n",
    "    min_lat = NEW_YORK_BOX_COORDS[0][0]\n",
    "    min_lon = NEW_YORK_BOX_COORDS[0][1]\n",
    "    max_lat = NEW_YORK_BOX_COORDS[1][0]\n",
    "    max_lon = NEW_YORK_BOX_COORDS[1][1]\n",
    "    \n",
    "    if ((from_coord[0] >= min_lat) & (from_coord[0] <= max_lat) \\\n",
    "        & (to_coord[0] >= min_lat) & (to_coord[0] <= max_lat) \\\n",
    "        & (from_coord[1] >= min_lon) & (from_coord[1] <= max_lon) \\\n",
    "        & (to_coord[1] >= min_lon) & (to_coord[1] <= max_lon)): \\\n",
    "        valid = True\n",
    "\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e9b12",
   "metadata": {},
   "source": [
    "## Process Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "339c47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean uber data\n",
    "def load_and_clean_uber_data(csv_file):\n",
    "    # load data\n",
    "    # remove the first 2 columns since they are useless ids and datetime we have as another column\n",
    "    uber = pd.read_csv(csv_file).iloc[:, 2:]\n",
    "    \n",
    "    # remove trips with unrealistic fare_amount and passenger_count \n",
    "    uber = uber[(uber[\"fare_amount\"] > 0) & (uber[\"passenger_count\"] < 8) & (uber[\"passenger_count\"] >= 1)]\n",
    "    \n",
    "    # select and rename relevant columns\n",
    "    uber.columns = [s.lower() for s in uber.columns]\n",
    "    uber = uber[[\"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]  \n",
    "     \n",
    "    uber.rename(columns={\"pickup_datetime\": \"start_datetime\", \\\n",
    "                         \"pickup_latitude\": \"start_lat\", \\\n",
    "                         \"pickup_longitude\": \"start_lon\", \\\n",
    "                         \"dropoff_latitude\": \"end_lat\", \\\n",
    "                         \"dropoff_longitude\": \"end_lon\"},\n",
    "               inplace=True)\n",
    "    \n",
    "    # format datetime as string\n",
    "    uber[\"start_datetime\"] = [x[:-4] for x in uber[\"start_datetime\"]]\n",
    "    \n",
    "    # round longitude & latitude to 6 decimals\n",
    "    cols = [\"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\"]\n",
    "    uber[cols] = uber[cols].round(6)\n",
    "    \n",
    "    # select trips in the valid New York range\n",
    "    # select valid trips\n",
    "    dt_sub = uber[[\"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\"]]\n",
    "    v = dt_sub.apply(lambda x: isValidTrip_coords(tuple([x[\"start_lat\"], x[\"start_lon\"]]), \\\n",
    "                                                  tuple([x[\"end_lat\"], x[\"end_lon\"]])), \\\n",
    "                     axis=1)\n",
    "    uber = uber[v]\n",
    "    \n",
    "    # rearrange the columns\n",
    "    uber = uber[[\"start_datetime\"]+cols]\n",
    "    \n",
    "    # sort by datetime, reset index\n",
    "    uber.sort_values(by=[\"start_datetime\"], inplace=True)\n",
    "    uber.reset_index(drop=True, inplace=True)\n",
    "    return uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "404b6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get(load, clean, add distance column) to uber data\n",
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    uber_dataframe = add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91ca18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uber data\n",
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e870434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194768, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 01:15:22</td>\n",
       "      <td>40.779456</td>\n",
       "      <td>-73.981918</td>\n",
       "      <td>40.771043</td>\n",
       "      <td>-73.957685</td>\n",
       "      <td>0.025652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:59:17</td>\n",
       "      <td>40.721389</td>\n",
       "      <td>-73.983759</td>\n",
       "      <td>40.687179</td>\n",
       "      <td>-73.994833</td>\n",
       "      <td>0.035958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:05:03</td>\n",
       "      <td>40.771254</td>\n",
       "      <td>-73.956635</td>\n",
       "      <td>40.749778</td>\n",
       "      <td>-73.991528</td>\n",
       "      <td>0.040972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 02:09:13</td>\n",
       "      <td>40.728020</td>\n",
       "      <td>-73.984605</td>\n",
       "      <td>40.776830</td>\n",
       "      <td>-73.955746</td>\n",
       "      <td>0.056703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 02:13:41</td>\n",
       "      <td>40.737425</td>\n",
       "      <td>-73.980127</td>\n",
       "      <td>40.726025</td>\n",
       "      <td>-74.009544</td>\n",
       "      <td>0.031549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start_datetime  start_lat  start_lon    end_lat    end_lon  distance\n",
       "0  2009-01-01 01:15:22  40.779456 -73.981918  40.771043 -73.957685  0.025652\n",
       "1  2009-01-01 01:59:17  40.721389 -73.983759  40.687179 -73.994833  0.035958\n",
       "2  2009-01-01 02:05:03  40.771254 -73.956635  40.749778 -73.991528  0.040972\n",
       "3  2009-01-01 02:09:13  40.728020 -73.984605  40.776830 -73.955746  0.056703\n",
       "4  2009-01-01 02:13:41  40.737425 -73.980127  40.726025 -74.009544  0.031549"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check uber_data\n",
    "print(uber_data.shape)\n",
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91491a7e",
   "metadata": {},
   "source": [
    "## Process taxi data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b85c730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all urls from taxi webpage\n",
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    response = requests.get(taxi_page)\n",
    "    html = response.content\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    all_urls = soup.find_all(\"a\")\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9a1c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter yellow taxi parquet urls\n",
    "def filter_taxi_parquet_urls(all_urls):\n",
    "    all_yellow_urls = []\n",
    "    pattern = r\"Yellow\\sTaxi\\sTrip\\sRecords$\"\n",
    "    for a in all_urls:\n",
    "        if re.search(pattern, a.text):\n",
    "            all_yellow_urls.append(a[\"href\"])\n",
    "    return sorted(all_yellow_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e56cb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample, clean, add distance column and finally save csv file for the given month\n",
    "def sample_clean_addDistance_month(yr, m):\n",
    "    # load the downloaded parquet file\n",
    "    data = pq.read_table(f\"{TAXI_DATA_DIR}/yellow_tripdata_{yr}_{m}.parquet\")\n",
    "        \n",
    "    # determine what column names the parquet data of this month uses\n",
    "    colnames_list = list([\"tpep_pickup_datetime\", \"PULocationID\", \"DOLocationID\"], \\\n",
    "                         [\"Trip_Pickup_DateTime\", \"Start_Lat\", \"Start_Lon\", \"End_Lat\", \"End_Lon\"], \\\n",
    "                         [\"pickup_datetime\", \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "    \n",
    "    if \"tpep_pickup_datetime\" in data.column_names:\n",
    "        colnames_type = 0 \n",
    "    elif \"Trip_Pickup_DateTime\" in data.column_names:\n",
    "        colnames_type = 1\n",
    "    else:\n",
    "        colnames_type = 2\n",
    "\n",
    "    cols = colnames_list[colnames_type]\n",
    "    \n",
    "    # sampling & select relevant columns\n",
    "    sample_idx = random.sample(list(range(len(data))), 200000)\n",
    "    sample_data = data.take(sample_idx).select(cols)\n",
    "        \n",
    "    # convert parquet table to pandas dataframe, throw rows with NA values\n",
    "    df = sample_data.to_pandas()\n",
    "            \n",
    "    if (colnames_type == 0):\n",
    "        # rename the columns, modify the data types\n",
    "        # convert timestamp to string\n",
    "        pattern = r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\"\n",
    "        ts = df[\"tpep_pickup_datetime\"].apply(lambda x: re.search(pattern, str(x)).group())\n",
    "        df[\"start_datetime\"] = ts\n",
    "        \n",
    "        # convert zone id to coords\n",
    "        from_coords = df[\"PULocationID\"].apply(lookup_coords_for_taxi_zone_id)\n",
    "        to_coords = df[\"DOLocationID\"].apply(lookup_coords_for_taxi_zone_id)\n",
    "            \n",
    "        start_lat_list, start_lon_list = list(zip(*from_coords))\n",
    "        end_lat_list, end_lon_list = list(zip(*to_coords))\n",
    "            \n",
    "        df[\"start_lat\"] = start_lat_list\n",
    "        df[\"start_lon\"] = start_lon_list\n",
    "        df[\"end_lat\"] = end_lat_list\n",
    "        df[\"end_lon\"] = end_lon_list\n",
    "        \n",
    "        df.sort_values(by = [\"start_datetime\"], inplace = True)\n",
    "        df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "    if (colnames_type==1):\n",
    "        # rename the columns\n",
    "        df.rename(columns = {\"Trip_Pickup_DateTime\": \"start_datetime\", \\\n",
    "                             \"Start_Lat\": \"start_lat\", \\\n",
    "                             \"Start_Lon\": \"start_lon\", \\\n",
    "                             \"End_Lat\": \"end_lat\", \\\n",
    "                             \"End_Lon\": \"end_lon\"}, \n",
    "                 inplace = True)\n",
    "        \n",
    "    if (colnames_type==2): \n",
    "        # rename the columns\n",
    "        df.rename(columns = {\"pickup_datetime\": \"start_datetime\", \\\n",
    "                             \"pickup_latitude\": \"start_lat\", \\\n",
    "                             \"pickup_longitude\": \"start_lon\", \\\n",
    "                             \"dropoff_latitude\": \"end_lat\", \\\n",
    "                             \"dropoff_longitude\": \"end_lon\"}, \n",
    "                 inplace = True)\n",
    "    \n",
    "    # select rows without NA values\n",
    "    df = df[~df.isnull()]\n",
    "    \n",
    "    # select valid trips\n",
    "    dt_sub = df[[\"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\"]]\n",
    "    v = dt_sub.apply(lambda x: isValidTrip_coords(tuple([x[\"start_lat\"], x[\"start_lon\"]]), \\\n",
    "                                                  tuple([x[\"end_lat\"], x[\"end_lon\"]])), \\\n",
    "                     axis = 1)\n",
    "    df = df[v]\n",
    "        \n",
    "    # add distance column\n",
    "    df = add_distance_column(df)\n",
    "    df = df[df[\"distance\"] > 0]\n",
    "    \n",
    "    # order columns by name\n",
    "    cols_ordered = [\"start_datetime\", \"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\", \"distance\"]\n",
    "    df = df[cols_ordered]\n",
    "    \n",
    "    # round longitudes, latitudes and distance to 6 decimals\n",
    "    cols = [\"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\", \"distance\"]\n",
    "    df[cols] = df[cols].round(6)\n",
    "    \n",
    "    # sort the dataframe by datetime, reset the index\n",
    "    df.sort_values(by = [\"start_datetime\"], inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # save the monthly dataframe\n",
    "    df.to_csv(f\"{TAXI_DATA_DIR}/yellow_tripdata_{yr}_{m}.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f4511b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample, clean, add distance column and finally save csv file for the given month\n",
    "def sample_clean_addDistance_month(yr, m):\n",
    "    # load the downloaded parquet file\n",
    "    data = pq.read_table(f\"{TAXI_DATA_DIR}/yellow_tripdata_{yr}_{m}.parquet\")\n",
    "        \n",
    "    # determine what column names the parquet data of this month uses\n",
    "    colnames_list = [[\"tpep_pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"tip_amount\"], \\\n",
    "                     [\"Trip_Pickup_DateTime\", \"Start_Lat\", \"Start_Lon\", \"End_Lat\", \"End_Lon\", \"Tip_Amt\"], \\\n",
    "                     [\"pickup_datetime\", \"pickup_latitude\", \"pickup_longitude\", \\\n",
    "                      \"dropoff_latitude\", \"dropoff_longitude\", \"tip_amount\"]]\n",
    "    \n",
    "    if \"tpep_pickup_datetime\" in data.column_names:\n",
    "        colnames_type = 0 \n",
    "    elif \"Trip_Pickup_DateTime\" in data.column_names:\n",
    "        colnames_type = 1\n",
    "    else:\n",
    "        colnames_type = 2\n",
    "\n",
    "    cols = colnames_list[colnames_type]\n",
    "    \n",
    "    # sampling & select relevant columns\n",
    "    sample_idx = random.sample(list(range(len(data))), 200000)\n",
    "    sample_data = data.take(sample_idx).select(cols)\n",
    "        \n",
    "    # convert parquet table to pandas dataframe, throw rows with NA values\n",
    "    df = sample_data.to_pandas()\n",
    "            \n",
    "    if (colnames_type == 0):\n",
    "        # rename the columns, modify the data types\n",
    "        # convert timestamp to string\n",
    "        pattern = r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\"\n",
    "        ts = df[\"tpep_pickup_datetime\"].apply(lambda x: re.search(pattern, str(x)).group())\n",
    "        df[\"start_datetime\"] = ts\n",
    "        \n",
    "        # convert zone id to coords\n",
    "        from_coords = df[\"PULocationID\"].apply(lookup_coords_for_taxi_zone_id)\n",
    "        to_coords = df[\"DOLocationID\"].apply(lookup_coords_for_taxi_zone_id)\n",
    "            \n",
    "        start_lat_list, start_lon_list = list(zip(*from_coords))\n",
    "        end_lat_list, end_lon_list = list(zip(*to_coords))\n",
    "            \n",
    "        df[\"start_lat\"] = start_lat_list\n",
    "        df[\"start_lon\"] = start_lon_list\n",
    "        df[\"end_lat\"] = end_lat_list\n",
    "        df[\"end_lon\"] = end_lon_list\n",
    "        \n",
    "        df.sort_values(by = [\"start_datetime\"], inplace = True)\n",
    "        df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "    if (colnames_type==1):\n",
    "        # rename the columns\n",
    "        df.rename(columns = {\"Trip_Pickup_DateTime\": \"start_datetime\", \\\n",
    "                             \"Start_Lat\": \"start_lat\", \\\n",
    "                             \"Start_Lon\": \"start_lon\", \\\n",
    "                             \"End_Lat\": \"end_lat\", \\\n",
    "                             \"End_Lon\": \"end_lon\", \\\n",
    "                             \"Tip_Amt\": \"tip_amount\"}, \n",
    "                 inplace = True)\n",
    "        \n",
    "    if (colnames_type==2): \n",
    "        # rename the columns\n",
    "        df.rename(columns = {\"pickup_datetime\": \"start_datetime\", \\\n",
    "                             \"pickup_latitude\": \"start_lat\", \\\n",
    "                             \"pickup_longitude\": \"start_lon\", \\\n",
    "                             \"dropoff_latitude\": \"end_lat\", \\\n",
    "                             \"dropoff_longitude\": \"end_lon\"}, \n",
    "                 inplace = True)\n",
    "    \n",
    "    # select rows without NA values\n",
    "    df = df[~df.isnull()]\n",
    "    \n",
    "    # select valid trips\n",
    "    dt_sub = df[[\"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\"]]\n",
    "    v = dt_sub.apply(lambda x: isValidTrip_coords(tuple([x[\"start_lat\"], x[\"start_lon\"]]), \\\n",
    "                                                  tuple([x[\"end_lat\"], x[\"end_lon\"]])), \\\n",
    "                     axis = 1)\n",
    "    df = df[v]\n",
    "        \n",
    "    # add distance column\n",
    "    df = add_distance_column(df)\n",
    "    df = df[df[\"distance\"] > 0]\n",
    "    \n",
    "    # order columns by name\n",
    "    cols_ordered = [\"start_datetime\", \"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\", \"distance\", \"tip_amount\"]\n",
    "    df = df[cols_ordered]\n",
    "    \n",
    "    # round longitudes, latitudes and distance to 6 decimals\n",
    "    cols = [\"start_lat\", \"start_lon\", \"end_lat\", \"end_lon\", \"distance\"]\n",
    "    df[cols] = df[cols].round(6)\n",
    "    df[\"tip_amount\"] = df[\"tip_amount\"].round(2)\n",
    "    \n",
    "    # sort the dataframe by datetime, reset the index\n",
    "    df.sort_values(by = [\"start_datetime\"], inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # save the monthly dataframe\n",
    "    df.to_csv(f\"{TAXI_DATA_DIR}/yellow_tripdata_{yr}_{m}.csv\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8186a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    yellow_tripdata_files = os.listdir(TAXI_DATA_DIR)\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # use named groups in regex to extract year and month\n",
    "        pattern = r\"/yellow_tripdata_(?P<year>\\d{4})-(?P<month>\\d{2})\"\n",
    "        match = re.search(pattern, parquet_url)\n",
    "        yr = match.group(\"year\")\n",
    "        m = match.group(\"month\")\n",
    "        \n",
    "        # only get and clean for valid months\n",
    "        if isValidYM(int(yr), int(m)): \n",
    "            # download parquet file with given url\n",
    "            # first check whether the parquet file has already been downloaded\n",
    "            parquet_fname = f\"yellow_tripdata_{yr}_{m}.parquet\"\n",
    "            if parquet_fname in yellow_tripdata_files: \n",
    "                pass\n",
    "            else: \n",
    "                response = requests.get(parquet_url, stream=True)\n",
    "                with open(f\"{TAXI_DATA_DIR}/yellow_tripdata_{yr}_{m}.parquet\", \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024): \n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "            \n",
    "            # cleaning and sampling\n",
    "            # first check whether we have cleaned data of this month\n",
    "            csv_fname = f\"yellow_tripdata_{yr}_{m}.csv\"\n",
    "            if csv_fname in yellow_tripdata_files: \n",
    "                df = pd.read_csv(f\"{TAXI_DATA_DIR}/yellow_tripdata_{yr}_{m}.csv\", index_col = 0)\n",
    "            else: \n",
    "                df = sample_clean_addDistance_month(yr, m)   \n",
    "            all_taxi_dataframes.append(df)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    taxi_data.sort_values(by = [\"start_datetime\"], inplace = True)\n",
    "    taxi_data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5553ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_yellow_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_yellow_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1694d698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for the first time: get, sample, clean, save csv, concatenate all dataframes\n",
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6832678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and concatenate from csv files after the first time of downloading and cleaning\n",
    "all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "all_yellow_urls = filter_taxi_parquet_urls(all_urls)\n",
    "\n",
    "yellow_taxi_csv_files = [s for s in os.listdir(TAXI_DATA_DIR) if s.endswith(\".csv\")]\n",
    "all_taxi_dataframes = []\n",
    "for fname in yellow_taxi_csv_files:\n",
    "    df = pd.read_csv(f\"{TAXI_DATA_DIR}/{fname}\", index_col=0)\n",
    "    all_taxi_dataframes.append(df)\n",
    "taxi_data = pd.concat(all_taxi_dataframes)\n",
    "taxi_data.sort_values(by = [\"start_datetime\"], inplace = True)\n",
    "taxi_data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20c2cb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14613478, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lon</th>\n",
       "      <th>distance</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:00:30</td>\n",
       "      <td>40.728857</td>\n",
       "      <td>-73.984596</td>\n",
       "      <td>40.722528</td>\n",
       "      <td>-73.986196</td>\n",
       "      <td>0.006528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 00:01:46</td>\n",
       "      <td>40.733459</td>\n",
       "      <td>-73.999581</td>\n",
       "      <td>40.734307</td>\n",
       "      <td>-73.990855</td>\n",
       "      <td>0.008767</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 00:02:04</td>\n",
       "      <td>40.743486</td>\n",
       "      <td>-73.996040</td>\n",
       "      <td>40.726504</td>\n",
       "      <td>-73.983240</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 00:04:43</td>\n",
       "      <td>40.759017</td>\n",
       "      <td>-73.992155</td>\n",
       "      <td>40.780733</td>\n",
       "      <td>-73.959001</td>\n",
       "      <td>0.039633</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 00:05:09</td>\n",
       "      <td>40.765080</td>\n",
       "      <td>-73.988076</td>\n",
       "      <td>40.740205</td>\n",
       "      <td>-73.996378</td>\n",
       "      <td>0.026224</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start_datetime  start_lat  start_lon    end_lat    end_lon  distance  \\\n",
       "0  2009-01-01 00:00:30  40.728857 -73.984596  40.722528 -73.986196  0.006528   \n",
       "1  2009-01-01 00:01:46  40.733459 -73.999581  40.734307 -73.990855  0.008767   \n",
       "2  2009-01-01 00:02:04  40.743486 -73.996040  40.726504 -73.983240  0.021266   \n",
       "3  2009-01-01 00:04:43  40.759017 -73.992155  40.780733 -73.959001  0.039633   \n",
       "4  2009-01-01 00:05:09  40.765080 -73.988076  40.740205 -73.996378  0.026224   \n",
       "\n",
       "   tip_amount  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check taxi_data\n",
    "print(taxi_data.shape) # should be slightly smaller than 200000*(12*6+6)=15600000, exactly 7 columns\n",
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc5470",
   "metadata": {},
   "source": [
    "## Process Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "334c667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all csv files under the given directory\n",
    "def get_all_weather_csvs(directory):\n",
    "    filenames_in_directory = sorted(os.listdir(directory))\n",
    "    pattern = r\"\\d{4}_weather.csv\"\n",
    "    weather_csv_files = []\n",
    "    for fname in filenames_in_directory:\n",
    "        if re.search(pattern, fname):\n",
    "            data = pd.read_csv(f\"{directory}/{fname}\")\n",
    "            weather_csv_files.append(data)\n",
    "    return weather_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07b8628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    # select relevant columns \n",
    "    data = csv_file[[\"DATE\", \"HourlyWindSpeed\", \"HourlyPrecipitation\"]]\n",
    "    \n",
    "    # format datetime object into \"%Y-%m-%d %H:%M:%S\"\n",
    "    # \"2009-01-01T00:51:00\" -> \"2009-01-01 00:51:00\"\n",
    "    data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
    "\n",
    "    # create dataframe with 365/366*24 datetime\n",
    "    \n",
    "    # get first datetime object\n",
    "    datetime_init = datetime.strptime(str(data[\"DATE\"][0]), \"%Y-%m-%d %H:%M:%S\")\n",
    "    # get 24 hours for the first day\n",
    "    firstday = []\n",
    "    for i in range(24):\n",
    "        firstday.append(datetime_init + timedelta(hours = i))\n",
    "    firstday = pd.DataFrame(firstday, columns=[\"DATE\"])\n",
    "    \n",
    "    # list of dataframes for every day\n",
    "    all_days = [firstday]\n",
    "        \n",
    "    # 366 days for leap year 2012, 181 days for first 6 months in 2015, 365 days for other years\n",
    "    if str(data[\"DATE\"][0]).startswith(\"2012\"):\n",
    "        n_days = 366\n",
    "    elif str(data[\"DATE\"][0]).startswith(\"2015\"):\n",
    "        n_days = 181\n",
    "    else:\n",
    "        n_days = 365\n",
    "        \n",
    "    # get 24 hours for every day starting from the second day\n",
    "    for n in range(1, n_days):\n",
    "        thisday = firstday[\"DATE\"].apply(lambda t: t + timedelta(days = n))\n",
    "        thisday = pd.DataFrame(thisday, columns=[\"DATE\"])\n",
    "        all_days.append(thisday)\n",
    "        \n",
    "    # concatenate all days\n",
    "    df = pd.concat(all_days, ignore_index=True)\n",
    "    # convert datetime object back to str\n",
    "    df[\"DATE\"] = df[\"DATE\"].astype(str)\n",
    "    \n",
    "    data = data.dropna(how=\"all\", subset=[\"HourlyWindSpeed\", \"HourlyPrecipitation\"])\n",
    "    res = df.merge(data, how=\"left\", on=[\"DATE\"])\n",
    "    res = res.drop_duplicates(subset=[\"DATE\"])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c0dcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    # select relevant columns \n",
    "    data = csv_file[[\"DATE\", \"DailyAverageWindSpeed\", \"DailyPrecipitation\"]]\n",
    "    \n",
    "    # format datetime object into \"%Y-%m-%d %H:%M:%S\"\n",
    "    # \"2009-01-01T00:51:00\" -> \"2009-01-01 00:51:00\"\n",
    "    data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
    "\n",
    "    # create dataframe with 365/366 datetime\n",
    "    \n",
    "    # get first datetime object\n",
    "    year = str(data[\"DATE\"][0])[:4]\n",
    "    df = pd.DataFrame({\"DATE\": [year+\"-01-01 23:59:00\"]})\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # 366 days for leap year 2012, 181 days for first 6 months in 2015, 365 days for other years\n",
    "    if year==\"2012\":\n",
    "        n_days = 366\n",
    "    elif year==\"2015\":\n",
    "        n_days = 181\n",
    "    else:\n",
    "        n_days = 365\n",
    "        \n",
    "    # get 24 hours for every day starting from the second day\n",
    "    for n in range(1, n_days):\n",
    "        df.loc[len(df.index)] = [df[\"DATE\"][0] + timedelta(days = n)]\n",
    "        \n",
    "    # convert datetime back to str\n",
    "    df[\"DATE\"] = df[\"DATE\"].astype(str)\n",
    "    \n",
    "    data = data.dropna(how=\"all\", subset=[\"DailyAverageWindSpeed\", \"DailyPrecipitation\"])\n",
    "    res = df.merge(data, how=\"left\", on=[\"DATE\"])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac21daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    hourly_data.rename(columns={\"DATE\": \"datetime\"}, inplace=True)\n",
    "    daily_data.rename(columns={\"DATE\": \"datetime\"}, inplace=True)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b69005b7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3764627406.py:8: DtypeWarning: Columns (9,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f\"{directory}/{fname}\")\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3764627406.py:8: DtypeWarning: Columns (8,9,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f\"{directory}/{fname}\")\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3764627406.py:8: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f\"{directory}/{fname}\")\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3764627406.py:8: DtypeWarning: Columns (7,8,9,10,17,18,42,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f\"{directory}/{fname}\")\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3764627406.py:8: DtypeWarning: Columns (17,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f\"{directory}/{fname}\")\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3764627406.py:8: DtypeWarning: Columns (8,9,17,18,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f\"{directory}/{fname}\")\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3764627406.py:8: DtypeWarning: Columns (10,41,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f\"{directory}/{fname}\")\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/458643598.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3231986597.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/458643598.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3231986597.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/458643598.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3231986597.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/458643598.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3231986597.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/458643598.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3231986597.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/458643598.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3231986597.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/458643598.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n",
      "/var/folders/0_/h7ztm2_n7r966r5fb1mr7dp00000gn/T/ipykernel_1978/3231986597.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"DATE\"] = data[\"DATE\"].apply(lambda s: s.replace(\"T\", \" \"))\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc526f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56928, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  HourlyWindSpeed HourlyPrecipitation\n",
       "0  2009-01-01 00:51:00             18.0                 NaN\n",
       "1  2009-01-01 01:51:00             18.0                 NaN\n",
       "2  2009-01-01 02:51:00             18.0                 NaN\n",
       "3  2009-01-01 03:51:00              8.0                 NaN\n",
       "4  2009-01-01 04:51:00             11.0                 NaN"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check hourly_weather_data\n",
    "print(hourly_weather_data.shape) # should have 24*(365*5+366+181)=56928 rows\n",
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7779e317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2372, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>DailyAverageWindSpeed</th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 23:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02 23:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-03 23:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-04 23:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-05 23:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  DailyAverageWindSpeed DailyPrecipitation\n",
       "0  2009-01-01 23:59:00                    NaN                NaN\n",
       "1  2009-01-02 23:59:00                    NaN                NaN\n",
       "2  2009-01-03 23:59:00                    NaN                NaN\n",
       "3  2009-01-04 23:59:00                    NaN                NaN\n",
       "4  2009-01-05 23:59:00                    NaN                NaN"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check daily_weather_data\n",
    "print(daily_weather_data.shape) # should have 365*5+366+181=2372 rows\n",
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf6943",
   "metadata": {},
   "source": [
    "# Part II: Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a51659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36b78f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather(\n",
    "    hourlyweatherId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    datetime DATE, \n",
    "    HourlyWindSpeed FLOAT,\n",
    "    HourlyPrecipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather(\n",
    "    dailyweatherId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    datetime DATE, \n",
    "    DailyAverageWindSpeed FLOAT,\n",
    "    DailyPrecipitation FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips(\n",
    "    taxitripsId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    start_datetime DATE, \n",
    "    start_lat FLOAT,\n",
    "    start_lon FLOAT,\n",
    "    end_lat FLOAT,\n",
    "    end_lon FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips(\n",
    "    ubertripsId INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    start_datetime DATE, \n",
    "    start_lat FLOAT,\n",
    "    start_lon FLOAT,\n",
    "    end_lat FLOAT,\n",
    "    end_lon FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f262d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de95414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffdeeef",
   "metadata": {},
   "source": [
    "## Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8abe8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for tablename, dataframe in table_to_df_dict.items():\n",
    "        dataframe.to_sql(tablename, con=engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "508a17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6eda635",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142f98b",
   "metadata": {},
   "source": [
    "# Part III: Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9072e7fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATABASE_URL\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query_stmt, query_filename):\n",
    "    with open(f\"{QUERY_DIRECTORY}/{query_filename}\", \"w\") as f:\n",
    "        f.write(query_stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cab86",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4956c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"query1\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT COUNT(*) FROM taxi_trips WHERE start_datetime LIKE \"% 00:%\";\n",
    "SELECT COUNT(*) FROM taxi_trips WHERE start_datetime LIKE \"% 01:%\";\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada8ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7fd84215",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'cursor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m()\u001b[38;5;241m.\u001b[39mexecutescript(QUERY_1)\u001b[38;5;241m.\u001b[39mfetchall()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'cursor'"
     ]
    }
   ],
   "source": [
    "engine.cursor().executescript(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33297c",
   "metadata": {},
   "source": [
    "# Part IV: Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b6572",
   "metadata": {},
   "source": [
    "## Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2f42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
